{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tadook/transcriptome_metab/blob/master/Deep_asthma_optuna_timm_squeezenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwQFZ__wtFLm"
      },
      "outputs": [],
      "source": [
        "# When using Google Colab GPU\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "id": "E-koar6ZwakI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/rwightman/pytorch-image-models.git"
      ],
      "metadata": {
        "id": "IS84Et3Vft5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbDYUdGrbxbf"
      },
      "outputs": [],
      "source": [
        "pip install -U --quiet wandb albumentations optuna torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "print(timm.__version__)\n",
        "from timm.scheduler import create_scheduler_v2\n",
        "\n",
        "timm.list_models(pretrained=True)"
      ],
      "metadata": {
        "id": "ixlCkRTRcyFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchsummary\n",
        "# m = timm.create_model(\"convnext_base\")\n",
        "#torchsummary.summary(m, (3, 224, 224), device=\"cpu\")"
      ],
      "metadata": {
        "id": "T9FM4_PlHFFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTYgLzkDoKDz"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "input_img_dir = \"/content/tramet/imgs_tramet_0506_82\" # shared drive\n",
        "\n",
        "train_dataset = ImageFolder(os.path.join(input_img_dir, \"train\"))\n",
        "val_dataset = ImageFolder(os.path.join(input_img_dir, \"test\")) # val\n",
        "test_dataset = ImageFolder(os.path.join(input_img_dir, \"test\"))\n",
        "\n",
        "train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppNrsLAftr9y"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "# wandb might cause an error without this.\n",
        "os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "\n",
        "\n",
        "def show(imgs, labels):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(30, 30))\n",
        "    for i, img in enumerate(imgs):\n",
        "        #img = img.detach()\n",
        "        #img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set_title(str(labels[i]))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "imgs = [train_dataset[i][0] for i in range(5)]\n",
        "labels = [train_dataset[i][1] for i in range(5)]\n",
        "show(imgs, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qe0yM8Urbca"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import wandb\n",
        "import timm.utils.random\n",
        "\n",
        "cudnn.benchmark = True\n",
        "timm.utils.random.random_seed(seed=42)\n",
        "\n",
        "config = {'lr': 1e-5, 'epochs': 500, 'batch_size': 32}\n",
        "\n",
        "mixup_args = {\n",
        "    'mixup_alpha': 1.,\n",
        "    'cutmix_alpha': 0.,\n",
        "    'cutmix_minmax': None,\n",
        "    'prob': .0,  # off\n",
        "    'switch_prob': 0.0,\n",
        "    'mode': 'batch',\n",
        "    'label_smoothing': 0.1,\n",
        "    'num_classes': 2}\n",
        "\n",
        "config.update(mixup_args)\n",
        "\n",
        "wandb.init(project='PROJECT NAME', entity='ENTITY NAME', config=config) # for weights&biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eelrxnyUjUl"
      },
      "outputs": [],
      "source": [
        "from timm.data.mixup import Mixup\n",
        "\n",
        "if mixup_args['prob'] > 0.0 and mixup_args['mixup_alpha'] > 0.0:\n",
        "  mixup_fn = Mixup(**mixup_args)\n",
        "else:\n",
        "  mixup_fn = None\n",
        "\n",
        "mixup_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgQhiVORwTyZ"
      },
      "outputs": [],
      "source": [
        "from albumentations.augmentations.transforms import MultiplicativeNoise\n",
        "from PIL import Image\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # add some augmentations\n",
        "        # lambda image: Image.fromarray(MultiplicativeNoise(multiplier=(0.8, 1.2), per_channel=True, elementwise=True, always_apply=False, p=0.5)(image=np.array(image))['image']),\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(input_img_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'test']}\n",
        "image_datasets['val'] = image_datasets['test']  # re-use test set\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=1, drop_last=(x == 'train'))\n",
        "              for x in ['train', 'val', 'test']}\n",
        "# drop_last for mixup during train\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUihDP0MwfmD"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize=(30, 30))\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "#    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "target_class = 0\n",
        "idx = classes == target_class\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs[idx])\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes[idx]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E3pwUCuVjW3"
      },
      "outputs": [],
      "source": [
        "target_class = 1\n",
        "idx = classes == target_class\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs[idx])\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes[idx]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEwsItpUUl2u"
      },
      "outputs": [],
      "source": [
        "print(classes)\n",
        "\n",
        "# Get a batch of training data\n",
        "if mixup_fn is not None:\n",
        "  mix_inputs, mix_classes = mixup_fn(inputs, classes)\n",
        "\n",
        "  # Make a grid from batch\n",
        "  out = torchvision.utils.make_grid(mix_inputs)\n",
        "\n",
        "  print(mix_classes)\n",
        "\n",
        "  imshow(out, title=[class_names[x] for x in mix_classes.argmax(1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZhncYhX2l-j"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "def evaluate(net, dataloader):\n",
        "  predicted_scores = np.empty(0)\n",
        "  true_labels = np.empty(0)\n",
        "  predicted_labels = np.empty(0)\n",
        "  with torch.no_grad():\n",
        "      net.eval()\n",
        "      for i, data in enumerate(dataloader):\n",
        "          inputs, labels = data\n",
        "          pred = torch.max(net(inputs.cuda()),1)[1].cpu().detach().numpy()\n",
        "          predicted_scores = np.append(predicted_scores, pred)\n",
        "          true_labels = np.append(true_labels, labels.cpu().detach().numpy())\n",
        "          predicted_labels = np.append(predicted_labels, (pred > 0.5).astype(int))\n",
        "\n",
        "  assert predicted_scores.shape == true_labels.shape\n",
        "  assert true_labels.shape == predicted_labels.shape\n",
        "\n",
        "  metrics = {\n",
        "      \"accuracy\": accuracy_score(true_labels, predicted_scores),\n",
        "      \"auroc\": roc_auc_score(true_labels, predicted_scores),\n",
        "      \"pred_pos_rate\": np.mean(predicted_labels),\n",
        "      \"true_pos_rate\": np.mean(true_labels),\n",
        "      \"n_samples\": len(true_labels)\n",
        "  }\n",
        "\n",
        "  return predicted_scores, true_labels, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_aiPUPEwsnA"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    # best_acc = 0.0\n",
        "    best_auroc = 0.0\n",
        "\n",
        "    # SWA/EMA https://github.com/rwightman/pytorch-image-models/blob/e8ddc6865c2c74871e6953e8d42873829768ba14/train.py\n",
        "    ema_model = None # timm.utils.ModelEmaV2(model)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'test', 'val']:\n",
        "          if ema_model is not None:\n",
        "            _, _, metrics = evaluate(ema_model.module, dataloaders[phase])\n",
        "          else:\n",
        "            _, _, metrics = evaluate(model, dataloaders[phase])\n",
        "\n",
        "          for k, v in metrics.items():\n",
        "            wandb.log({f'{phase}_{k}': v}, step=epoch)\n",
        "\n",
        "          if phase == 'val':\n",
        "            val_auroc = metrics['auroc']\n",
        "            print(\"val auroc\", val_auroc)\n",
        "            if best_auroc < val_auroc:\n",
        "                best_auroc = val_auroc\n",
        "                if ema_model is not None:\n",
        "                  best_model_wts = copy.deepcopy(ema_model.module.state_dict())\n",
        "                else:\n",
        "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                print(f\"best score {best_auroc}\")\n",
        "                print(\"saving model weights\")\n",
        "                torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                if mixup_fn is not None:\n",
        "                  inputs, labels = mixup_fn(inputs, labels)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    with torch.autocast(\"cuda\"):\n",
        "                      outputs = model(inputs)\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if ema_model is not None:\n",
        "                           ema_model.update(model)\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                if mixup_fn is not None:\n",
        "                  labels_max = labels.argmax(1)\n",
        "                  running_corrects += torch.sum(preds == labels_max)\n",
        "                else:\n",
        "                  running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step(epoch)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            wandb.log({f'{phase}_loss': epoch_loss, 'lr': optimizer.param_groups[0][\"lr\"]}, step=epoch)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best test auroc: {best_auroc:4f}')\n",
        "\n",
        "    # https://wandb.ai/wandb/common-ml-errors/reports/How-to-Save-and-Load-Models-in-PyTorch--VmlldzozMjg0MTE\n",
        "    torch.save(model.state_dict(), \"last_model.pth\")\n",
        "\n",
        "    artifact = wandb.Artifact('last_model', type='model')\n",
        "    artifact.add_file('last_model.pth')\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "    artifact = wandb.Artifact('best_model', type='model')\n",
        "    artifact.add_file('best_model.pth')\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "    import datetime\n",
        "    datetime_str = datetime.datetime.fromtimestamp(wandb.run.start_time).strftime('%Y-%m-%d-%H-%M-%S')\n",
        "\n",
        "    # drive_save_dir = f\"/content/drive/MyDrive/tramet_models/{datetime_str}_{wandb.run.name}\"\n",
        "    drive_save_dir = f\"/content/tramet/tramet_models/{datetime_str}_{wandb.run.name}\"\n",
        "    os.makedirs(drive_save_dir, exist_ok=True)\n",
        "    import shutil\n",
        "    shutil.copy('last_model.pth', os.path.join(drive_save_dir, 'last_model.pth'))\n",
        "    shutil.copy('best_model.pth', os.path.join(drive_save_dir, 'best_model.pth'))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T5mFAz8xAjD"
      },
      "outputs": [],
      "source": [
        "def create_resnet(dropout_rate=0.5):\n",
        "  model_ft = models.resnet50(pretrained=True)\n",
        "  num_ftrs = model_ft.fc.in_features\n",
        "\n",
        "  if dropout_rate > 0.0:\n",
        "      model_ft.fc = nn.Sequential(\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(num_ftrs, 2)\n",
        "        )\n",
        "  else:\n",
        "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "  return model_ft\n",
        "\n",
        "\n",
        "def create_squeezenet():\n",
        "  model_ft = models.squeezenet1_1(pretrained=True)\n",
        "  model_ft.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1,1), stride=(1,1))\n",
        "\n",
        "  return model_ft\n",
        "\n",
        "\n",
        "def create_model_and_optim(model_name='resnet50', lr=1e-5, drop_rate=0.5, num_epochs=1000, weight_decay=0.1, layer_decay=0.75):\n",
        "  if model_name == 'torchvision_resnet50':\n",
        "    model_ft = create_resnet()\n",
        "\n",
        "  elif model_name == 'squeezenet':\n",
        "    model_ft = create_squeezenet()\n",
        "\n",
        "  else:\n",
        "    import timm\n",
        "    # from timm.models.layers.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
        "    model_ft = timm.create_model(model_name, pretrained=True, num_classes=2, drop_rate=drop_rate) # , global_pool='')\n",
        "    # model_ft.fc = nn.Sequential(\n",
        "    #           nn.Conv2d(model_ft.num_features, 512, 7, bias=True),\n",
        "    #           nn.BatchNorm2d(512),\n",
        "    #           nn.ReLU(),\n",
        "    #           nn.Dropout(0.5),\n",
        "    #           nn.Conv2d(512, 512, 1, bias=True),\n",
        "    #           nn.ReLU(),\n",
        "    #           nn.Dropout(0.5),\n",
        "    #           SelectAdaptivePool2d(pool_type='avg', flatten=True),\n",
        "    #           nn.Linear(512, 2, bias=True)\n",
        "    #   )\n",
        "\n",
        "    # model_ft.set_grad_checkpointing(True)\n",
        "\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  # weighted\n",
        "  from sklearn.utils.class_weight import compute_class_weight\n",
        "  train_y = np.array(image_datasets['train'].targets)\n",
        "  class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_y), y=train_y)\n",
        "  class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "  print(\"class_weights\", class_weights)\n",
        "\n",
        "  from timm.loss import SoftTargetCrossEntropy, BinaryCrossEntropy\n",
        "\n",
        "  # criterion = nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
        "  # criterion = SoftTargetCrossEntropy()\n",
        "  criterion = BinaryCrossEntropy(smoothing=0.1, # target_threshold: Optional[float] = None, weight: Optional[torch.Tensor] = None,\n",
        "              reduction = 'mean', pos_weight = class_weights)\n",
        "\n",
        "  from timm.optim import create_optimizer_v2, optimizer_kwargs\n",
        "  optimizer_ft = create_optimizer_v2(model_ft.parameters(), opt='AdamW', lr=lr, weight_decay=weight_decay, layer_decay=layer_decay)\n",
        "  # optimizer_ft = optim.Adam(model_ft.parameters(), lr=lr)\n",
        "\n",
        "  #scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=500, gamma=0.5)\n",
        "  from timm.scheduler import create_scheduler_v2\n",
        "  scheduler, num_epochs = create_scheduler_v2(optimizer_ft, warmup_lr=1e-9, warmup_epochs=50, num_epochs=num_epochs)\n",
        "\n",
        "  return model_ft, optimizer_ft, scheduler, criterion\n",
        "\n",
        "\n",
        "#model_ft, optimizer_ft, scheduler, criterion = create_model_and_optim('resnet152')\n",
        "#torchsummary.summary(model_ft, (3, 224, 224), device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbORm2TVxHt4"
      },
      "outputs": [],
      "source": [
        "# model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=wandb.config.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEcqv-p9zCEc"
      },
      "outputs": [],
      "source": [
        "#train_predicted, train_true, train_metrics = evaluate(model_ft, dataloaders['train'])\n",
        "#print(train_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPbUoCRBzTkq"
      },
      "outputs": [],
      "source": [
        "# run predictions on test samples\n",
        "#test_predicted, test_true, test_metrics = evaluate(model_ft, dataloaders['test'])\n",
        "#print(test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzRH1M8yzabk"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  params = {\n",
        "      'lr': trial.suggest_categorical('lr', [8e-6]), # manual search from 1e-7 to 1e-5; 8e-6[best@main analysis], 4e-6[best@sensitivity analysis]\n",
        "      'epochs': trial.suggest_categorical('epochs', [1500]),\n",
        "      'encoder': trial.suggest_categorical('encoder', ['squeezenet']), #, 'tv_resnet152', 'swin_large_patch4_window7_224_in22k'])\n",
        "      'drop_rate': trial.suggest_categorical('drop_rate', [None]), # 0.25,\n",
        "      'layer_decay': trial.suggest_categorical('layer_decay', [None]), # .25, 0.5, 0.9\n",
        "      'weight_decay': trial.suggest_categorical('weight_decay', [0.01]), # 0.1, 0.001\n",
        "  }\n",
        "\n",
        "  config = dict(trial.params)\n",
        "  config[\"trial.number\"] = trial.number\n",
        "  print(config)\n",
        "  wandb.init(\n",
        "      project=\"PROJECT NAME\",\n",
        "      entity='ENTITY NAME',\n",
        "      config=config,\n",
        "      group=\"0506_squeezenet_save\",\n",
        "      reinit=True,\n",
        "  )\n",
        "\n",
        "  timm.utils.random.random_seed(seed=42)\n",
        "\n",
        "  model_ft, optimizer_ft, scheduler, criterion = create_model_and_optim(model_name=params['encoder'], num_epochs=params['epochs'], lr=params['lr'], drop_rate=params['drop_rate'], layer_decay=params['layer_decay'], weight_decay=params['weight_decay'])\n",
        "\n",
        "  torchsummary.summary(model_ft, (3, 224, 224), device=\"cuda\")\n",
        "\n",
        "  model_ft = train_model(model_ft, criterion, optimizer_ft, scheduler, num_epochs=params['epochs'])\n",
        "\n",
        "  test_predicted, test_true, test_metrics = evaluate(model_ft, dataloaders['test'])\n",
        "  print(test_metrics)\n",
        "  wandb.log({\"final_test_\" + k: v for k, v in test_metrics.items()})\n",
        "\n",
        "  val_predicted, val_true, val_metrics = evaluate(model_ft, dataloaders['val'])\n",
        "  print(val_metrics)\n",
        "  wandb.log({\"final_val_\" + k: v for k, v in val_metrics.items()})\n",
        "\n",
        "  primary_metric = val_metrics['auroc']\n",
        "  wandb.run.summary[\"final val auroc\"] = primary_metric\n",
        "  wandb.run.summary[\"state\"] = \"completed\"\n",
        "  wandb.finish(quiet=True)\n",
        "\n",
        "  return primary_metric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    pruner=optuna.pruners.MedianPruner(),\n",
        ")\n",
        "study.optimize(objective, n_trials=1, timeout=120000000)"
      ],
      "metadata": {
        "id": "IwKcFTRQ0PAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model, _, _, _ = create_model_and_optim(\"squeezenet\")\n",
        "\n",
        "loaded_model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "loaded_model.load_state_dict(torch.load('last_model.pth'))"
      ],
      "metadata": {
        "id": "w96x3p7r0inB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}